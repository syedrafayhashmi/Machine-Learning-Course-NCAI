{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was scrapped using scrapping tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches=pd.read_csv('./data/PSL_2016-2020.csv')\n",
    "matches.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NO NULL VALUES FOUND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = {'Team1_Code': {'KK':1,'LQ':2,'MS':3,'IU':4,'PZ':5 ,'QG':6},\n",
    "          'Team2_Code': {'KK':1,'LQ':2,'MS':3,'IU':4,'PZ':5 ,'QG':6},\n",
    "          'toss_winner_code': {'KK':1,'LQ':2,'MS':3,'IU':4,'PZ':5 ,'QG':6},\n",
    "          'winner_code': {'KK':1,'LQ':2,'MS':3,'IU':4,'PZ':5 ,'QG':6,'MT':7,'MA':8},\n",
    "         'toss_decision': {'field':1,'bat':2}}\n",
    "matches.replace(encode,inplace=True)\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Finding The Team code the key or value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicVal = encode['winner_code']\n",
    "print(dicVal['QG']) #key value\n",
    "print(list(dicVal.keys())[list(dicVal.values()).index(6)]) #find key by value search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(matches)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Total Winnig by each team in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = df[\"winner_code\"]\n",
    "tot_wins = dict(wins.value_counts())\n",
    "tot_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['team1_total_wins'] = df['Team1_Code'].map(tot_wins)\n",
    "df['team2_total_wins'] = df['Team2_Code'].map(tot_wins)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting The Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches[['Team1_Code','Team2_Code','toss_winner_code','toss_decision','winner_code',\"team1_total_wins\",\"team2_total_wins\"]]\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding out the Total Number of Wins by each Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=df['toss_winner_code'].value_counts(sort=True)\n",
    "temp2=df['winner_code'].value_counts(sort=True)\n",
    "#'No of toss winners by each team'\n",
    "print('No of toss winners by each team')\n",
    "for idx, val in temp1.iteritems():\n",
    "    print('{} -> {}'.format(list(dicVal.keys())[list(dicVal.values()).index(idx)],val))\n",
    "print('No of match winners by each team')\n",
    "for idx, val in temp2.iteritems():\n",
    "    print('{} -> {}'.format(list(dicVal.keys())[list(dicVal.values()).index(idx)],val))\n",
    "print(\"Where MA : Match Abandoned\\n      MT: Match Tied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HISTOGRAM OF TEAM WINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['winner_code'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shows the Team labeled 6(i.e Quetta Gladiators) won most of the matches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the toss wins and match wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_xlabel('Team')\n",
    "ax1.set_ylabel('Count of toss wins')\n",
    "ax1.set_title(\"toss winners\")\n",
    "temp1.plot(kind='bar')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "temp2.plot(kind = 'bar')\n",
    "ax2.set_xlabel('Team')\n",
    "ax2.set_ylabel('count of matches won')\n",
    "ax2.set_title(\"Match winners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking If there is any null value in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.apply(lambda x: sum(x.isnull()),axis=0) \n",
    "    #find the null values in every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold creates multiple random splits of the dataset into training and validation data.\n",
    "For each such split, the model is fit to the training data,\n",
    "and predictive accuracy is assessed using the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold   #For K-fold cross validation\n",
    "from sklearn import metrics\n",
    "\n",
    "def classification_model(model, data, predictors, outcome):\n",
    "    model.fit(data[predictors],data[outcome])\n",
    "    predictions = model.predict(data[predictors])\n",
    "    accuracy = metrics.accuracy_score(predictions,data[outcome])\n",
    "    print('Accuracy : %s' % '{0:.3%}'.format(accuracy))\n",
    "    kf = KFold(n_splits=7,random_state=0)\n",
    "    error = []\n",
    "    for train, test in kf.split(data[predictors]):\n",
    "        train_predictors = (data[predictors].iloc[train,:])\n",
    "        train_target = data[outcome].iloc[train]\n",
    "        model.fit(train_predictors, train_target)\n",
    "        error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n",
    " \n",
    "    print('Cross-Validation Score : %s' % '{0:.3%}'.format(np.mean(error)))\n",
    "\n",
    "    model.fit(data[predictors],data[outcome]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### To avoid any warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=60)\n",
    "outcome_var = ['winner_code']\n",
    "predictor_var = ['Team1_Code','Team2_Code','toss_winner_code','toss_decision','team1_total_wins','team2_total_wins']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1='IU'\n",
    "team2='QG'\n",
    "toss_winner='QG'\n",
    "input=[dicVal[team1],dicVal[team2],dicVal[toss_winner],'2','27','30']\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=df['toss_winner_code'].value_counts(sort=True)\n",
    "temp2=df['winner_code'].value_counts(sort=True)\n",
    "#'No of toss winners by each team'\n",
    "print('No of toss winners by each team')\n",
    "for idx, val in temp1.iteritems():\n",
    "    print('{} -> {}'.format(list(dicVal.keys())[list(dicVal.values()).index(idx)],val))\n",
    "print('No of match winners by each team')\n",
    "for idx, val in temp2.iteritems():\n",
    "    print('{} -> {}'.format(list(dicVal.keys())[list(dicVal.values()).index(idx)],val))\n",
    "print(\"Where MA : Match Abandoned\\n      MT: Match Tied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "field: 1 bat 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKING PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1='IU'\n",
    "team2='QG'\n",
    "toss_winner='QG'\n",
    "input=[dicVal[team1],dicVal[team2],dicVal[toss_winner],'2','27','30']\n",
    "input = np.array(input).reshape((1, -1))\n",
    "output=model.predict(input)\n",
    "winner = list(dicVal.keys())[list(dicVal.values()).index(output)] #find key by value search output\n",
    "print(f\"The Winner is {winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1='QG'\n",
    "team2='PZ'\n",
    "toss_winner='QG'\n",
    "input=[dicVal[team1],dicVal[team2],dicVal[toss_winner],'1','30','28']\n",
    "\n",
    "input = np.array(input).reshape((1, -1))\n",
    "output=model.predict(input)\n",
    "winner = list(dicVal.keys())[list(dicVal.values()).index(output)] #find key by value search output\n",
    "print(f\"The Winner is {winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1='KK'\n",
    "team2='LQ'\n",
    "toss_winner='LQ'\n",
    "input=[dicVal[team1],dicVal[team2],dicVal[toss_winner],'1','18','10']\n",
    "\n",
    "input = np.array(input).reshape((1, -1))\n",
    "output=model.predict(input)\n",
    "winner = list(dicVal.keys())[list(dicVal.values()).index(output)] #find key by value search output\n",
    "print(f\"The Winner is {winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1='IU'\n",
    "team2='KK'\n",
    "toss_winner='KK'\n",
    "input=[dicVal[team1],dicVal[team2],dicVal[toss_winner],'1','27','18']\n",
    "\n",
    "input = np.array(input).reshape((1, -1))\n",
    "output=model.predict(input)\n",
    "winner = list(dicVal.keys())[list(dicVal.values()).index(output)] #find key by value search output\n",
    "print(f\"The Winner is {winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important features of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_input = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\n",
    "print(imp_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, feature_names):\n",
    "    import numpy as np\n",
    "    c_features = len(feature_names)\n",
    "    plt.barh(range(c_features), model.feature_importances_)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature name\")\n",
    "    plt.yticks(np.arange(c_features), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_input = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\n",
    "print(imp_input)\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.title(\"Model's Feature Importance\")\n",
    "plot_feature_importances(model, predictor_var)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['winner_code']]\n",
    "X = df[['Team1_Code','Team2_Code','toss_winner_code','toss_decision','team1_total_wins','team2_total_wins']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)\n",
    "knn.fit(X_train, y_train)\n",
    "print('Psl dataset: KNN')\n",
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of KNN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1,20)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"K value VS accuracy\")\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.xticks([0,5,10,15,20]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for s in t:\n",
    "\n",
    "    scores = []\n",
    "    for i in range(1,1000):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s)\n",
    "        knn.fit(X_train, y_train)\n",
    "        scores.append(knn.score(X_test, y_test))\n",
    "    plt.plot(s, np.mean(scores), 'bo')\n",
    "plt.title(\"Training set proportion VS Accuracy\")\n",
    "plt.xlabel('Training set proportion (%)')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "y = df[['winner_code']]\n",
    "X = df[['Team1_Code','Team2_Code','toss_winner_code','toss_decision','team1_total_wins','team2_total_wins']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier(random_state=0,max_depth=5, max_leaf_nodes=5,min_samples_leaf=5)\n",
    "DT.fit(X_train, y_train)\n",
    "print('Psl dataset: DT')\n",
    "print('Accuracy of DT classifier on training set: {:.2f}'\n",
    "     .format(DT.score(X_train, y_train)))\n",
    "print('Accuracy of DT classifier on test set: {:.2f}'\n",
    "     .format(DT.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_nodes_range = range(2,6)\n",
    "scores = []\n",
    "\n",
    "for x in leaf_nodes_range:\n",
    "    DT = DecisionTreeClassifier(random_state=0,max_depth=5, max_leaf_nodes=x,min_samples_leaf=5)\n",
    "    DT.fit(X_train, y_train)\n",
    "    scores.append(DT.score(X_test, y_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Leaf Nodes VS Accuracy\")\n",
    "plt.xlabel('nodes')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(leaf_nodes_range, scores)\n",
    "plt.xticks([1,2,3,4,5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_range = range(1,6)\n",
    "scores = []\n",
    "\n",
    "for x in depth_range:\n",
    "    DT = DecisionTreeClassifier(random_state=0,max_depth=x, max_leaf_nodes=13,min_samples_leaf=5)\n",
    "    DT.fit(X_train, y_train)\n",
    "    scores.append(DT.score(X_test, y_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Depth VS Accuracy\")\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(depth_range, scores)\n",
    "plt.xticks([0,1,2,3,4,5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "plt.figure()\n",
    "for s in t:\n",
    "    scores = []\n",
    "    for i in range(1,1000):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s)\n",
    "        DT.fit(X_train, y_train)\n",
    "        scores.append(DT.score(X_test, y_test))\n",
    "    plt.plot(s, np.mean(scores), 'bo')\n",
    "plt.title(\"Training set proportion VS Accuracy\")\n",
    "plt.xlabel('Training set proportion (%)')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "y = df[['winner_code']]\n",
    "X = df[['Team1_Code','Team2_Code','toss_winner_code','toss_decision','team1_total_wins','team2_total_wins']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(random_state=0,max_depth=4)\n",
    "RF.fit(X_train, y_train)\n",
    "print('Psl dataset:RF')\n",
    "print('Accuracy of RF classifier on training set: {:.2f}'\n",
    "     .format(RF.score(X_train, y_train)))\n",
    "print('Accuracy of RF classifier on test set: {:.2f}'\n",
    "     .format(RF.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_range = range(1,6)\n",
    "scores = []\n",
    "\n",
    "for z in depth_range:\n",
    "    RF = RandomForestClassifier(random_state=0,max_depth=z)\n",
    "    RF.fit(X_train, y_train)\n",
    "    scores.append(RF.score(X_test, y_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Depth VS Accuracy\")\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(depth_range, scores)\n",
    "plt.xticks([0,1,2,3,4,5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "plt.figure()\n",
    "for s in t:\n",
    "    scores = []\n",
    "    for i in range(1,1000):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s)\n",
    "        DT.fit(X_train, y_train)\n",
    "        scores.append(DT.score(X_test, y_test))\n",
    "    plt.plot(s, np.mean(scores), 'bo')\n",
    "plt.title(\"Training set proportion VS Accuracy\")\n",
    "plt.xlabel('Training set proportion (%)')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using KFold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "y = df[['winner_code']]\n",
    "X = df[['Team1_Code','Team2_Code','toss_winner_code','toss_decision','team1_total_wins','team2_total_wins']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold   #For K-fold cross validation\n",
    "from sklearn import metrics\n",
    "\n",
    "def classification_model(model, data, predictors, outcome):\n",
    "    model.fit(data[predictors],data[outcome])\n",
    "    predictions = model.predict(data[predictors])\n",
    "    accuracy = metrics.accuracy_score(predictions,data[outcome])\n",
    "    print('Accuracy : %s' % '{0:.3%}'.format(accuracy))\n",
    "    kf = KFold(n_splits=7,random_state=None)\n",
    "    error = []\n",
    "    for train, test in kf.split(data[predictors]):\n",
    "        train_predictors = (data[predictors].iloc[train,:])\n",
    "        train_target = data[outcome].iloc[train]\n",
    "        model.fit(train_predictors, train_target)\n",
    "        error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n",
    " \n",
    "    print('Cross-Validation Score : %s' % '{0:.3%}'.format(np.mean(error)))\n",
    "\n",
    "    model.fit(data[predictors],data[outcome]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors = 1)\n",
    "outcome_var = ['winner_code']\n",
    "predictor_var = ['Team1_Code','Team2_Code','toss_winner_code','toss_decision','team1_total_wins','team2_total_wins']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree with cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "outcome_var = ['winner_code']\n",
    "predictor_var = ['Team1_Code','Team2_Code','toss_winner_code','toss_decision','team1_total_wins','team2_total_wins']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=60)\n",
    "outcome_var = ['winner_code']\n",
    "predictor_var = ['Team1_Code','Team2_Code','toss_winner_code','toss_decision','team1_total_wins','team2_total_wins']\n",
    "classification_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
